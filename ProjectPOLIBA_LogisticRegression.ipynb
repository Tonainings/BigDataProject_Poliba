{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0Iuq/O59L88dhsTGMPGKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tonainings/BigDataProject_Poliba/blob/main/ProjectPOLIBA_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o2kuV9yIhalO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"_student_prediction.csv\")\n",
        "\n",
        "\n",
        "#x_train = np.array(df[\"CUML_GPA\"])   #una variabile da 145 valori\n",
        "#y_train = np.array(df[\"GRADE\"])\n",
        "\n",
        "print(df.info())\n",
        "\n",
        "#X = np.array([df[\"PREP_STUDY\"],df[\"PREP_EXAM\"],df[\"NOTES\"],df[\"LISTENS\"]])       # 4 variabili da 145 valori per ognuno\n",
        "#Y = np.array(df[\"GRADE\"])\n",
        "#W = np.array([-4.0, 1.0, 1.0, 1.0])\n",
        "#b = 0\n"
      ],
      "metadata": {
        "id": "qDcCGh0afkqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492bbb41-fa71-4d1d-dce9-b29a0746cdd0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 145 entries, 0 to 144\n",
            "Data columns (total 33 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   STUDENTID      145 non-null    object\n",
            " 1   AGE            145 non-null    int64 \n",
            " 2   GENDER         145 non-null    int64 \n",
            " 3   HS_TYPE        145 non-null    int64 \n",
            " 4   SCHOLARSHIP    145 non-null    int64 \n",
            " 5   WORK           145 non-null    int64 \n",
            " 6   ACTIVITY       145 non-null    int64 \n",
            " 7   PARTNER        145 non-null    int64 \n",
            " 8   SALARY         145 non-null    int64 \n",
            " 9   TRANSPORT      145 non-null    int64 \n",
            " 10  LIVING         145 non-null    int64 \n",
            " 11  MOTHER_EDU     145 non-null    int64 \n",
            " 12  FATHER_EDU     145 non-null    int64 \n",
            " 13  #_SIBLINGS     145 non-null    int64 \n",
            " 14  KIDS           145 non-null    int64 \n",
            " 15  MOTHER_JOB     145 non-null    int64 \n",
            " 16  FATHER_JOB     145 non-null    int64 \n",
            " 17  STUDY_HRS      145 non-null    int64 \n",
            " 18  READ_FREQ      145 non-null    int64 \n",
            " 19  READ_FREQ_SCI  145 non-null    int64 \n",
            " 20  ATTEND_DEPT    145 non-null    int64 \n",
            " 21  IMPACT         145 non-null    int64 \n",
            " 22  ATTEND         145 non-null    int64 \n",
            " 23  PREP_STUDY     145 non-null    int64 \n",
            " 24  PREP_EXAM      145 non-null    int64 \n",
            " 25  NOTES          145 non-null    int64 \n",
            " 26  LISTENS        145 non-null    int64 \n",
            " 27  LIKES_DISCUSS  145 non-null    int64 \n",
            " 28  CLASSROOM      145 non-null    int64 \n",
            " 29  CUML_GPA       145 non-null    int64 \n",
            " 30  EXP_GPA        145 non-null    int64 \n",
            " 31  COURSE ID      145 non-null    int64 \n",
            " 32  GRADE          145 non-null    int64 \n",
            "dtypes: int64(32), object(1)\n",
            "memory usage: 37.5+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df.sort_values(by=['GRADE'], ascending= TRUE, implace= TRUE)                     #sort by\n",
        "\n",
        "df.head(10)  #mostra i primi 10 record\n"
      ],
      "metadata": {
        "id": "u6LDXeKBjGXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#X1 = df[['PREP_STUDY']].to_numpy()\n",
        "X1 = df[['PREP_STUDY', 'PREP_EXAM', 'NOTES', 'LISTENS']].to_numpy()                 #converte dataframe in array\n",
        "Y1 = df[['GRADE']].to_numpy()\n",
        "\n",
        "X = np.array(X1, dtype=float)                                                       #converte i valori della variabile in float64\n",
        "#X2 = np.array(X1, dtype=float)\n",
        "#X = X2.transpose()                                                                 #scambia le righe con le colonne di un array, sicuramente funziona sia con matrici che con vettori\n",
        "y = np.array(Y1, dtype=float)\n",
        "#Y2 = np.array(Y1, dtype=float)\n",
        "#Y = Y2.transpose()\n",
        "\n",
        "W = np.array([-4.0, 1.0, 1.0, 1.0])\n",
        "b = 0\n",
        "\n",
        "#print(X)\n",
        "#print(y)\n",
        "#print(W)\n",
        "\n",
        "#print(type(X))                                                                      #stampa il tipo di variabile\n",
        "#print(X.dtype)                                                                      #stampa il formato dei valori di una variabile\n",
        "#print(type(y))\n",
        "#print(y.dtype)\n",
        "#print(type(W))\n",
        "#print(W.dtype)\n",
        "\n",
        "#x_train = np.array(df[\"CUML_GPA\"])   #una variabile da 145 valori\n",
        "#y_train = np.array(df[\"GRADE\"])\n",
        "\n",
        "#X = np.array([df[\"PREP_STUDY\"],df[\"PREP_EXAM\"],df[\"NOTES\"],df[\"LISTENS\"]])       # 4 variabili da 145 valori per ognuno\n",
        "#Y = np.array(df[\"GRADE\"])\n",
        "\n",
        "#W = np.array([-4.0, 1.0, 1.0, 1.0])\n",
        "#b = 0\n",
        "\n",
        "#print(x_train)\n",
        "#print(y_train)\n"
      ],
      "metadata": {
        "id": "icjJLGpb29nE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost compute"
      ],
      "metadata": {
        "id": "SA5b_IHsbU8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "def sig(x):\n",
        " return 1/(1 + np.exp(-x))\n",
        "\n",
        "def compute_multiple_cost(X, y, W, b):\n",
        "\n",
        "  m = X.shape[0]\n",
        "  cost = 0\n",
        "\n",
        "  for i in range(m):\n",
        "    z_i = np.dot(X[i],W) + b\n",
        "    f_xi = sig(z_i)\n",
        "    cost += -y[i]*np.log(f_xi) - (1-y[i])*np.log(1-f_xi)\n",
        "  total_cost = (1/(2*m))*cost\n",
        "  return total_cost\n",
        "\n",
        "total_cost = compute_multiple_cost(X, y, W, b)\n",
        "\n",
        "\n",
        "\n",
        "print ('for b={}, total_cost={}'.format(b,total_cost))"
      ],
      "metadata": {
        "id": "M0uCaQHEVMEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d89ba33-9194-45fe-bfe7-e0434c295186"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for b=0, total_cost=[-0.02334572]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqK4mF2-hXj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient compute"
      ],
      "metadata": {
        "id": "JQN3KtCgbc_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "pIfw3TkzgMHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f395a0d5-6cd4-40cf-9e1e-a4df99be5e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.68126628 -3.10743215 -6.61884156 -5.42787515] [-2.59764199]\n"
          ]
        }
      ],
      "source": [
        "def compute_multiple_gradient(X, y, W, b):\n",
        "\n",
        "  m = X.shape[0] # Number of training samples\n",
        "  n = X.shape[1] # Number of feature classes\n",
        "\n",
        "  dj_dW = np.zeros(n)\n",
        "  dj_db = 0\n",
        "\n",
        "  for i in range(m): # For each sample\n",
        "    z_i = np.dot(X[i],W) + b\n",
        "    f_xi = sig(z_i) # Prediction\n",
        "    err_i = f_xi - y[i] # Prediction - Ground truth\n",
        "    for j in range(n): # For each feature\n",
        "      x_ji = X[i,j]\n",
        "      dj_dW[j] = dj_dW[j] + (1/m)*x_ji*err_i\n",
        "    dj_db = dj_db + (1/m)*err_i\n",
        "  return dj_dW, dj_db\n",
        "\n",
        "dj_dW, dj_db = compute_multiple_gradient(X, y, W, b)\n",
        "print(dj_dW, dj_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent"
      ],
      "metadata": {
        "id": "_osbRLIcb6T4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1-R7BEecgUSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c99c33-94e6-44cf-c7d6-7fbc39ee9db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: (w,b) = ([-3.99928935  1.00059873  1.00127409  1.0010464 ], [1.00049939]), cost = [-1.30512187]\n",
            "Iteration 100: (w,b) = ([-3.92913246  1.05977719  1.12717547  1.10446488], [1.04984733]), cost = [-2.24020143]\n",
            "Iteration 200: (w,b) = ([-3.86067452  1.11774168  1.2504752   1.20574745], [1.09817301]), cost = [-3.13621092]\n",
            "Iteration 300: (w,b) = ([-3.79370917  1.17472548  1.37173209  1.30531977], [1.14568361]), cost = [-4.00132743]\n",
            "Iteration 400: (w,b) = ([-3.72801743  1.23093456  1.49143279  1.40356984], [1.19255541]), cost = [-4.8426152]\n",
            "Iteration 500: (w,b) = ([-3.66345984  1.2865014   1.60990134  1.50077517], [1.23890767]), cost = [-5.6647686]\n",
            "Iteration 600: (w,b) = ([-3.59995505  1.34150287  1.72733058  1.5971252 ], [1.2848159]), cost = [-6.47070097]\n",
            "Iteration 700: (w,b) = ([-3.53739806  1.39601357  1.84388333  1.69278377], [1.33034666]), cost = [-7.2629984]\n",
            "Iteration 800: (w,b) = ([-3.47562742  1.45012749  1.95974635  1.78791305], [1.37557431]), cost = [-8.04456006]\n",
            "Iteration 900: (w,b) = ([-3.41446012  1.50394271  2.07510788  1.8826598 ], [1.42057335]), cost = [-8.81823635]\n",
            "Iteration 1000: (w,b) = ([-3.35372673  1.55754548  2.19012659  1.97714384], [1.46540908]), cost = [-9.58644043]\n",
            "Iteration 1100: (w,b) = ([-3.29328455  1.61100589  2.30492441  2.07145785], [1.51013494]), cost = [-10.35106737]\n",
            "Iteration 1200: (w,b) = ([-3.23302499  1.6643766   2.41958792  2.16566822], [1.55479113]), cost = [-11.11349171]\n",
            "Iteration 1300: (w,b) = ([-3.17287435  1.71769344  2.5341733   2.25981812], [1.59940499]), cost = [-11.87461911]\n",
            "Iteration 1400: (w,b) = ([-3.11278693  1.77097879  2.64871432  2.35393352], [1.6439937]), cost = [-12.63500108]\n",
            "Iteration 1500: (w,b) = ([-3.05273587  1.82424594  2.76323038  2.44802935], [1.68856757]), cost = [-13.3949579]\n",
            "Iteration 1600: (w,b) = ([-2.99270574  1.87750256  2.87773237  2.54211405], [1.73313267]), cost = [-14.15467161]\n",
            "Iteration 1700: (w,b) = ([-2.93268776  1.93075305  2.99222636  2.63619234], [1.77769254]), cost = [-14.91424516]\n",
            "Iteration 1800: (w,b) = ([-2.87267692  1.98399993  3.10671576  2.7302669 ], [1.82224927]), cost = [-15.67373702]\n",
            "Iteration 1900: (w,b) = ([-2.8126703   2.03724467  3.2212025   2.82433926], [1.8668041]), cost = [-16.43318068]\n",
            "Iteration 2000: (w,b) = ([-2.75266622  2.09048811  3.33568766  2.91841031], [1.91135776]), cost = [-17.19259557]\n",
            "Iteration 2100: (w,b) = ([-2.69266369  2.14373078  3.45017189  3.01248057], [1.9559107]), cost = [-17.95199308]\n",
            "Iteration 2200: (w,b) = ([-2.6326621   2.19697296  3.56465555  3.10655034], [2.00046319]), cost = [-18.71138]\n",
            "Iteration 2300: (w,b) = ([-2.5726611   2.25021485  3.67913888  3.20061982], [2.0450154]), cost = [-19.47076043]\n",
            "Iteration 2400: (w,b) = ([-2.51266047  2.30345655  3.79362199  3.29468911], [2.08956744]), cost = [-20.23013664]\n",
            "Iteration 2500: (w,b) = ([-2.45266006  2.35669813  3.90810497  3.38875828], [2.13411936]), cost = [-20.98951054]\n",
            "Iteration 2600: (w,b) = ([-2.3926598   2.40993964  4.02258787  3.48282737], [2.17867122]), cost = [-21.74888314]\n",
            "Iteration 2700: (w,b) = ([-2.33265964  2.4631811   4.13707072  3.57689642], [2.22322302]), cost = [-22.50825483]\n",
            "Iteration 2800: (w,b) = ([-2.27265953  2.51642254  4.25155353  3.67096544], [2.2677748]), cost = [-23.26762243]\n",
            "Iteration 2900: (w,b) = ([-2.21265946  2.56966395  4.36603633  3.76503444], [2.31232656]), cost = [-24.02699934]\n",
            "Iteration 3000: (w,b) = ([-2.15265942  2.62290535  4.48051911  3.85910343], [2.35687831]), cost = [-24.78638021]\n",
            "Iteration 3100: (w,b) = ([-2.09265939  2.67614675  4.59500189  3.95317241], [2.40143004]), cost = [-25.54574014]\n",
            "Iteration 3200: (w,b) = ([-2.03265937  2.72938814  4.70948465  4.04724138], [2.44598178]), cost = [-26.3051199]\n",
            "Iteration 3300: (w,b) = ([-1.97265936  2.78262952  4.82396742  4.14131036], [2.49053351]), cost = [-27.06446271]\n",
            "Iteration 3400: (w,b) = ([-1.91265935  2.8358709   4.93845018  4.23537933], [2.53508524]), cost = [-27.82395852]\n",
            "Iteration 3500: (w,b) = ([-1.85265935  2.88911229  5.05293294  4.32944829], [2.57963696]), cost = [-28.58377174]\n",
            "Iteration 3600: (w,b) = ([-1.79265934  2.94235367  5.1674157   4.42351726], [2.62418869]), cost = [-29.34507691]\n",
            "Iteration 3700: (w,b) = ([-1.73265934  2.99559505  5.28189846  4.51758623], [2.66874041]), cost = [-30.10699222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-1f8d74f25a7b>:13: RuntimeWarning: divide by zero encountered in log\n",
            "  cost += -y[i]*np.log(f_xi) - (1-y[i])*np.log(1-f_xi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3800: (w,b) = ([-1.67265934  3.04883643  5.39638122  4.61165519], [2.71329214]), cost = [-inf]\n",
            "Iteration 3900: (w,b) = ([-1.61265934  3.10207781  5.51086398  4.70572416], [2.75784386]), cost = [-inf]\n",
            "Iteration 4000: (w,b) = ([-1.55265934  3.15531919  5.62534674  4.79979313], [2.80239559]), cost = [-inf]\n",
            "Iteration 4100: (w,b) = ([-1.49265934  3.20856057  5.7398295   4.89386209], [2.84694731]), cost = [-inf]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-1f8d74f25a7b>:13: RuntimeWarning: invalid value encountered in multiply\n",
            "  cost += -y[i]*np.log(f_xi) - (1-y[i])*np.log(1-f_xi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4200: (w,b) = ([-1.43265934  3.26180195  5.85431226  4.98793106], [2.89149904]), cost = [nan]\n",
            "Iteration 4300: (w,b) = ([-1.37265934  3.31504333  5.96879502  5.08200002], [2.93605076]), cost = [nan]\n",
            "Iteration 4400: (w,b) = ([-1.31265934  3.3682847   6.08327778  5.17606899], [2.98060249]), cost = [nan]\n",
            "Iteration 4500: (w,b) = ([-1.25265934  3.42152608  6.19776053  5.27013795], [3.02515421]), cost = [nan]\n",
            "Iteration 4600: (w,b) = ([-1.19265934  3.47476746  6.31224329  5.36420692], [3.06970593]), cost = [nan]\n",
            "Iteration 4700: (w,b) = ([-1.13265934  3.52800884  6.42672605  5.45827588], [3.11425766]), cost = [nan]\n",
            "Iteration 4800: (w,b) = ([-1.07265934  3.58125022  6.54120881  5.55234485], [3.15880938]), cost = [nan]\n",
            "Iteration 4900: (w,b) = ([-1.01265934  3.6344916   6.65569157  5.64641382], [3.20336111]), cost = [nan]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.95325934,  3.68720057,  6.7690295 ,  5.73954209]),\n",
              " array([3.24746731]),\n",
              " array([nan]))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "def multiple_gradient_descent(X, y,  W_in, b_in, alpha,  num_iters,  cost_function, gradient_function):\n",
        "\n",
        "  W = W_in\n",
        "  b = b_in\n",
        "\n",
        "  for itt in range(num_iters):\n",
        "    dj_dw, dj_db = compute_multiple_gradient(X, y, W, b)\n",
        "    W = W - alpha*dj_dw\n",
        "    b = b - alpha*dj_db\n",
        "    cost = compute_multiple_cost(X, y, W, b)\n",
        "\n",
        "    if itt % 100 == 0:\n",
        "      print('Iteration {}: (w,b) = ({}, {}), cost = {}'.format(itt, W, b, cost))\n",
        "\n",
        "  return W, b, cost\n",
        "\n",
        "W_in = np.array([-4.0, 1.0, 1.0, 1.0])\n",
        "b_in = 1\n",
        "\n",
        "alpha = 0.0002\n",
        "num_iters = 5000\n",
        "\n",
        "multiple_gradient_descent(X, y,  W_in, b_in, alpha,  num_iters, compute_multiple_cost, compute_multiple_gradient)"
      ]
    }
  ]
}